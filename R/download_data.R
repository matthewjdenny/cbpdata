#' @title Download Bills Data
#' @description Downloads the bill metadata and text.
#'
#' @param bill_list A list object generated by the `get_bill_list()` function.
#' @param existing_data An optional data frame with existing bills data. The
#' `URL` and `last_updated` fields in these data will be used to filter which
#' observations are new/have changed, and only new data will be downloaded if
#' this is the case.
#' @param chunk_size Defaults to 1000. If `intermediate_directory` is not NULL,
#' then documents will be saved in chunks of 1000 to the intermediate directory,
#' then read back in to R when the process is complete. This allows for the user
#' to save results along the way to mitigate the effects of crashes.
#' @param intermediate_directory Defaults to NULL. If not NULL, then the data
#' will be downloaded and saved in chunks of `chunk_size` to this directory.
#' Make sure that a trailing "/" is used on this directory so that it can be
#' joined to the intermediate file names, which will be of the form:
#' "Data_Chunk_i.Rdata" where "i" is the chunk number.
#' @param sleep_time The number of seconds to sleep in between downloading new
#' data from the govinfo.gov website. Defaults to 1.
#' @param status_interval Defaults to 100. The interval (number of bills) at
#' which progress should be reported.
#' @param start_chunk If `intermediate_directory` is not NULL (so chunking is
#' being used), then this determines which chunk to start with. This can be
#' helpful for splitting scraping up across multiple machines. Defaults to 1.
#' @param recombine_data Logical, defaults to TRUE. If TRUE, and
#' `intermediate_directory` is provided, then at the end of scraping, all
#' datasets will be read in and recombined. If FALSE, then no further action
#' will be taken and NULL will be returned by the function:
#' @return A data.frame containing the raw xml/html returned by the web scraper,
#' along with relevant metadata.
#' @export
download_data <- function(bill_list,
                          existing_data = NULL,
                          chunk_size = 1000,
                          intermediate_directory = NULL,
                          sleep_time = 1,
                          status_interval = 100,
                          start_chunk = 1,
                          recombine_data = TRUE) {

    start <- Sys.time()

    # get number of items to download:
    n_download <- length(bill_list$year)

    # create a data frame to hold results
    to_return <- data.frame(bill_metadata_url = bill_list$bill_metadata,
                            bill_text_url = bill_list$bill_text,
                            year_introduced = bill_list$year,
                            date_last_modified = bill_list$last_modified,
                            metadata_request_status = rep("",n_download),
                            metadata_xml = rep("",n_download),
                            bill_text_request_status = rep("",n_download),
                            bill_text_html = rep("",n_download),
                            stringsAsFactors = FALSE)

    if (is.null(intermediate_directory)) {
        # if no intermediate directory is provided, then data are downloaded
        # directly in the the to_return data.frame:
        for (i in 1:nrow(to_return)) {
            if (i %% status_interval == 0) {
                cat("Currently working on Bill",i,"of",nrow(to_return),
                    "\nwith URL:",to_return$bill_metadata_url[i],"\n")
            }
            page <- httr::GET(to_return$bill_metadata_url[i])
            to_return$metadata_xml[i] <- httr::content(page, "text")
            to_return$metadata_request_status[i] <- page$status_code

            if (page$status_code != 200) {
                cat("-------- Potental Error ---------",
                    "\nPage:",to_return$bill_metadata_url[i],
                    "\nStatus Code:",page$status_code,"\n")
            }

            page <- httr::GET(to_return$bill_text_url[i])
            to_return$bill_text_html[i] <- httr::content(page, "text")
            to_return$bill_text_request_status[i] <- page$status_code

            if (page$status_code != 200) {
                cat("-------- Potental Error ---------",
                    "\nPage:",to_return$bill_text_url[i],
                    "\nStatus Code:",page$status_code,"\n")
            }

            Sys.sleep(sleep_time)
        }
    } else {
        # if an intermediate directory is provided, then we save chunks of up to
        # chunk size at a time to the intermediate directory:
        chunks <- ceiling(nrow(to_return) / chunk_size)

        for (i in start_chunk:chunks) {
            # need to create the current chunk:
            cur_start <- 1 + (i - 1) * chunk_size
            cur_end <- min(i * chunk_size, nrow(to_return))

            # create the subset of the data to operate on:
            cur <- to_return[cur_start:cur_end,]

            # does not return anything
            download_data_chunk(scraped_data = cur,
                                intermediate_directory = intermediate_directory,
                                sleep_time = sleep_time,
                                chunk_number = i,
                                status_interval = status_interval)
        }

        # if we are recombining the data:
        if (recombine_data) {
            # NULL out for package checks:
            scraped_data <- NULL
            for (i in start_chunk:chunks) {
                cat("Loading in chunk:",i,"of",chunks,"\n")
                filename <- paste(intermediate_directory,"Data_Chunk_",
                                  i,".RData",sep = "")
                # load in the intermediate data:
                load(filename)
                # put it in the data frame:
                to_return[cur_start:cur_end,] <- scraped_data
            }
        } else {
            return(NULL)
        }
    }

    end <- Sys.time()
    cat("Full Download complete with:\n")
    print(end - start)

    # return everything:
    return(to_return)
}
